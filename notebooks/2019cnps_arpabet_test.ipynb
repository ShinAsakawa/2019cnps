{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_arpabet_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otOQdA95u89J",
        "colab_type": "text"
      },
      "source": [
        "# ARPABET の活用\n",
        "\n",
        "もっとも簡単には `nltk` パッケージを活用して以下のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGe52M1Tu7b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk  # nltk パッケージの輸入\n",
        "nltk.download('cmudict')  # CMU 辞書のダウンロード クラウド上ではその都度必要です\n",
        "cmu = nltk.corpus.cmudict.dict()  # ダウンロードした辞書の準備\n",
        "print(cmu['apple']) # 'apple' の発音を調べてみます"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNeCP9P3vMfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 引き続き cmu 辞書を使って見ます\n",
        "sentence = 'Neural networks are awesome'.lower().split()\n",
        "print(sentence)  # 最初に入力文の印字\n",
        "for word in sentence:\n",
        "    print(word, ':--->', cmu[word])  # 各単語の発音を印字"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzTypnNSzNQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 以下は `nltk` からの蘊蓄です。\n",
        "\n",
        "# Natural Language Toolkit: Carnegie Mellon Pronouncing Dictionary Corpus Reader\n",
        "#\n",
        "# Copyright (C) 2001-2019 NLTK Project\n",
        "# Author: Steven Bird <stevenbird1@gmail.com>\n",
        "# URL: <http://nltk.org/>\n",
        "# For license information, see LICENSE.TXT\n",
        "\n",
        "\"\"\"\n",
        "The Carnegie Mellon Pronouncing Dictionary [cmudict.0.6]\n",
        "ftp://ftp.cs.cmu.edu/project/speech/dict/\n",
        "Copyright 1998 Carnegie Mellon University\n",
        "\n",
        "File Format: Each line consists of an uppercased word, a counter\n",
        "(for alternative pronunciations), and a transcription.  Vowels are\n",
        "marked for stress (1=primary, 2=secondary, 0=no stress).  E.g.:\n",
        "NATURAL 1 N AE1 CH ER0 AH0 L\n",
        "\n",
        "The dictionary contains 127069 entries.  Of these, 119400 words are assigned\n",
        "a unique pronunciation, 6830 words have two pronunciations, and 839 words have\n",
        "three or more pronunciations.  Many of these are fast-speech variants.\n",
        "\n",
        "Phonemes: There are 39 phonemes, as shown below:\n",
        "\n",
        "Phoneme Example Translation    Phoneme Example Translation\n",
        "------- ------- -----------    ------- ------- -----------\n",
        "AA      odd     AA D           AE      at      AE T\n",
        "AH      hut     HH AH T        AO      ought   AO T\n",
        "AW      cow     K AW           AY      hide    HH AY D\n",
        "B       be      B IY           CH      cheese  CH IY Z\n",
        "D       dee     D IY           DH      thee    DH IY\n",
        "EH      Ed      EH D           ER      hurt    HH ER T\n",
        "EY      ate     EY T           F       fee     F IY\n",
        "G       green   G R IY N       HH      he      HH IY\n",
        "IH      it      IH T           IY      eat     IY T\n",
        "JH      gee     JH IY          K       key     K IY\n",
        "L       lee     L IY           M       me      M IY\n",
        "N       knee    N IY           NG      ping    P IH NG\n",
        "OW      oat     OW T           OY      toy     T OY\n",
        "P       pee     P IY           R       read    R IY D\n",
        "S       sea     S IY           SH      she     SH IY\n",
        "T       tea     T IY           TH      theta   TH EY T AH\n",
        "UH      hood    HH UH D        UW      two     T UW\n",
        "V       vee     V IY           W       we      W IY\n",
        "Y       yield   Y IY L D       Z       zee     Z IY\n",
        "ZH      seizure S IY ZH ER\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR-4VtxHzAaQ",
        "colab_type": "text"
      },
      "source": [
        "もう少し精緻な処理をしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVGM0vddvnsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: http://compling.hss.ntu.edu.sg/courses/hg2051/code/wk5b.py\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "###  Find the 50 most frequent bigrams (see Week 6) in Jane Austen's Emma.\n",
        "###  Then find the 50 most frequent bigrams that do not include a stopword.\n",
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "\n",
        "print (\"Find the 50 most frequent bigrams in emma\")\n",
        "print ([b[0] for b in nltk.FreqDist(nltk.bigrams(emma)).most_common(50)])\n",
        "\n",
        "print (\"Find the 50 most frequent bigrams in emma without stopwords\")\n",
        "\n",
        "stopen = stopwords.words('english')\n",
        "\n",
        "# that do not contain stopwords (or punct)\n",
        "print (nltk.FreqDist(nltk.bigrams(w for w in emma \n",
        "                                 if w not in stopen \n",
        "                                 and w.isalpha())).most_common(50))\n",
        "\n",
        "### find the bigrams first then remove stop words\n",
        "print (\"Find the 50 most frequent bigrams without stopwords in emma\")\n",
        "print (nltk.FreqDist((w1, w2) for (w1, w2) in nltk.bigrams(w for w in emma\n",
        "                                                          if w.isalpha())\n",
        "                     if w1 not in stopen and w2  not in stopen).most_common(50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FUCWsTKxpxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmu['awesome']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI6VCyJnyEfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###\n",
        "### A Pronouncing Dictionary\n",
        "###\n",
        "pron = nltk.corpus.cmudict.entries()\n",
        "\n",
        "print(\"First ten entries\", pron[:10])\n",
        "\n",
        "# Find python in the Pronouncing Dictionary and print its pronunciation.\n",
        "python_pron = [p for w,p in pron if w =='python']\n",
        "print(\"Python is pronounced\", python_pron)\n",
        "\n",
        "# Find marathon and print its pronunciation.\n",
        "marathon_pron = [p for w,p in pron if w =='marathon']\n",
        "print(\"Marathon is pronounced\", marathon_pron)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjxm-Bvg1AcY",
        "colab_type": "text"
      },
      "source": [
        "# 続いて音素関係の処理と表示です"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg8S37zR0X0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find all the words whose last syllable rhymes with python.\n",
        "# Find all the words whose last syllable rhymes with marathon. \n",
        "\n",
        "for word in ['python', 'marathon']:\n",
        "    wp = [p for w,p in pron if w ==word]\n",
        "    print(\"{} is pronounced {}\".format(word, wp))\n",
        "    print(\"These words rhyme with it (assuming last three phonemes are ok):\")\n",
        "    print([(w,p[-3:]) for w,p in pron if len(p)>2 and wp[0][-3:]==p[-3:]])\n",
        "\n",
        "## A better definition of \"rhyme\" is \n",
        "### \"identical in pronunciation from the main-stressed vowel to the end\",\n",
        "### See: http://languagelog.ldc.upenn.edu/nll/?p=1946"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlH2c2IN1E1T",
        "colab_type": "text"
      },
      "source": [
        "# WordNet を使った類義語の処理と表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi5ku71v0h3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n",
        "\n",
        "### And of course: http://en.wiktionary.org/wiki/Rhymes:English\n",
        "\n",
        "# â˜… Write a function that converts Arbabet to IPA\n",
        "# Use it to print the pronunciations of python and marathon\n",
        "\n",
        "# Load wordnet inside python.\n",
        "from nltk.corpus import wordnet as wn\n",
        "print (\"\\nWordnet\\n\")\n",
        "\n",
        "#     Look at the different synsets for bird.\n",
        "print (\"Synsets for bird:\")\n",
        "print (wn.synsets('bird'))\n",
        "\n",
        "#     How many are there?\n",
        "print (\"# of senses for bird:\", len(wn.synsets('bird')))\n",
        "\n",
        "#     How deep in the hierarchy and what are the definitions?\n",
        "for s in wn.synsets('bird'):\n",
        "    print (s.name(), s.min_depth(), s.definition())\n",
        "\n",
        "\n",
        "print(\"\\nWhich  languages have lemmas for 'bird.n.01'?\")\n",
        "for l in wn.langs():\n",
        "    ### bug in Croation and Bulgarian, my bad\n",
        "    # if l in ('hrv', 'bul'):\n",
        "    #     continue ## skip to the next language\n",
        "    if wn.synset('bird.n.01').lemmas(lang=l):\n",
        "        print (l, end=': ')\n",
        "        print(\",\".join(wn.synset('bird.n.01').lemma_names(lang=l)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLJa4Ryn1NGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each synset, print out each lemma and its frequency (hint freqency of a lemma is given by lemma.count)\n",
        "for s in wn.synsets('bird'):\n",
        "    print(s)\n",
        "    for l in s.lemmas():\n",
        "        print (l.name(), l.count())\n",
        "    print()\n",
        "    \n",
        "# Give the total frequency for each synset \n",
        "for s in wn.synsets('bird'):\n",
        "    print (s.name(), sum(l.count() for l in s.lemmas()), s.definition())\n",
        "    \n",
        "\n",
        "# â˜… Tabulate the average polysemy per word length for all words in wordnet, and then seperately for each part of speech. (Hint: polysemy is number of synsets/word; you can get all words by [w for w in wn.all_lemma_names()]; for just nouns you can do: [w for w in wn.all_lemma_names('n')]. ) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvkjRyyZ1iok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lastsyllable (pron):\n",
        "    \"take the pronunciation from cmudict, return the last syllable\"\n",
        "    # e.g. ['P', 'AY1', 'TH', 'AA0', 'N'] -> ['TH', 'AA0', 'N']\n",
        "    end = []\n",
        "    nrop = pron[::-1]\n",
        "    if len(pron) ==1:\n",
        "        ## 'I'\n",
        "        return pron\n",
        "    for l in nrop:\n",
        "        end.insert(0,l)\n",
        "        ## stop when we come to a vowel\n",
        "        if l[-1].isdigit() and nrop.index(l) != 0:\n",
        "            ## add the precceding phoneme as well if there is one\n",
        "            ## don't do this if it is the second vowel 'Hammer'\n",
        "            if len(end) < len(nrop) and \\\n",
        "                len([c for c in end if c[-1].isdigit()]) < 2:  \n",
        "                end.insert(0,nrop[len(end)])\n",
        "            return end\n",
        "\n",
        "def rhymes(word, pdict):\n",
        "    for pron in pdict[word]:\n",
        "        print (\"Rhymes for:\", word, pron, lastsyllable(pron))\n",
        "        end = lastsyllable(pron)\n",
        "        for w, prons in pdict.items():\n",
        "            for p in prons:\n",
        "                if p[-len(end):] == end and w != word:\n",
        "                    print (w, p)\n",
        "        print()\n",
        "\n",
        "rhymes('neural', cmu)\n",
        "rhymes('networks', cmu)\n",
        "rhymes('are', cmu)\n",
        "rhymes('awesome', cmu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyww6E-K1_7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = ['cognitive', 'neuro', 'psychology', 'is', 'great', 'as', 'well']\n",
        "print(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaHxDFpM20uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in sentence:\n",
        "    print(rhymes(word,cmu))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}