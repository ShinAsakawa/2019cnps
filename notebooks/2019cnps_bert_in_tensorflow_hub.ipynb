{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019cnps_bert_in_tensorflow_hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_bert_in_tensorflow_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5vpaWQ-Pq1I",
        "colab_type": "text"
      },
      "source": [
        "Original: <https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1>\n",
        "\n",
        "# BERT 実習\n",
        "\n",
        "\n",
        "## Overview\n",
        "This module contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" [1].\n",
        "\n",
        "This module assumes pre-processed inputs from the BERT repository (https://github.com/google-research/bert)\n",
        "\n",
        "This modules outputs a representations for every token in the input sequence and a pooled representation of the entire input.\n",
        "\n",
        "## Trainable parameters\n",
        "All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice.\n",
        "\n",
        "## Example use\n",
        "Please see https://github.com/google-research/bert/blob/master/run_classifier_with_tfhub.py for how the input preprocessing should be done to retrieve the input ids, masks, and segment ids.\n",
        "\n",
        "```python\n",
        "bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True)\n",
        "bert_inputs = dict(\n",
        "    input_ids=input_ids,\n",
        "    input_mask=input_mask,\n",
        "    segment_ids=segment_ids)\n",
        "bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "pooled_output = bert_outputs[\"pooled_output\"]\n",
        "sequence_output = bert_outputs[\"sequence_output\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClvLVv4bPpq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6VEWli6P0g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True)\n",
        "bert_inputs = dict(\n",
        "    input_ids=input_ids,\n",
        "    input_mask=input_mask,\n",
        "    segment_ids=segment_ids)\n",
        "bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "pooled_output = bert_outputs[\"pooled_output\"]\n",
        "sequence_output = bert_outputs[\"sequence_output\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0jV2YxTQEBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}