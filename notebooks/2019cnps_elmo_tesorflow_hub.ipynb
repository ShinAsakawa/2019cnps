{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019cnps_elmo_tesorflow_hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_elmo_tesorflow_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_fAuw1HIO-9",
        "colab_type": "text"
      },
      "source": [
        "Original: <https://tfhub.dev/google/elmo/2>\n",
        "Paper: <https://arxiv.org/abs/1802.05365>\n",
        "Project: <https://allennlp.org/elmo>\n",
        "\n",
        "# エルモ ELMo の実習\n",
        "\n",
        "- Embeddings from a language model trained on the 1 Billion Word Benchmark.\n",
        "\n",
        "## Overview\n",
        "Computes contextualized word representations using character-based word representations and bidirectional LSTMs, as described in the paper \"Deep contextualized word representations\" [^1].\n",
        "\n",
        "This modules supports inputs both in the form of raw text strings or tokenized text strings.\n",
        "\n",
        "The module outputs fixed embeddings at each LSTM layer, a learnable aggregation of the 3 layers, and a fixed mean-pooled vector representation of the input.\n",
        "\n",
        "The complex architecture achieves state of the art results on several benchmarks. Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups. The use of an accelerator is recommended.\n",
        "\n",
        "### Trainable parameters\n",
        "The module exposes 4 trainable scalar weights for layer aggregation.\n",
        "\n",
        "### Example use\n",
        "\n",
        "```python\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "embeddings = elmo(\n",
        "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
        "                [\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
        "tokens_length = [6, 5]\n",
        "embeddings = elmo(\n",
        "    inputs={\n",
        "        \"tokens\": tokens_input,\n",
        "        \"sequence_len\": tokens_length\n",
        "    },\n",
        "    signature=\"tokens\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "```\n",
        "\n",
        "We set the trainable parameter to True when creating the module so that the 4 scalar weights (as described in the paper) can be trained. In this setting, the module still keeps all other parameters fixed.\n",
        "\n",
        "### Inputs\n",
        "The module defines [two signatures](https://www.tensorflow.org/hub/basics#applying_a_module): default, and tokens.\n",
        "\n",
        "With the default signature, the module takes untokenized sentences as input. The input tensor is a string tensor with shape `[batch_size]`. The module tokenizes each string by **splitting on spaces**.\n",
        "\n",
        "With the tokens signature, the module takes tokenized sentences as input. The input tensor is a string tensor with shape `[batch_size, max_length]` and an int32 tensor with shape `[batch_size]` corresponding to the sentence length. The length input is necessary to exclude padding in the case of sentences with varying length.\n",
        "\n",
        "### Outputs\n",
        "The output dictionary contains:\n",
        "\n",
        "* `word_emb`: the character-based word representations with shape `[batch_size, max_length, 512]`.\n",
        "* `lstm_outputs1`: the first LSTM hidden state with shape `[batch_size, max_length, 1024]`.\n",
        "* `lstm_outputs2`: the second LSTM hidden state with shape `[batch_size, max_length, 1024]`.\n",
        "* `elmo`: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape `[batch_size, max_length, 1024]`\n",
        "* `default`: a fixed mean-pooling of all contextualized word representations with shape `[batch_size, 1024]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTYQChXuFG5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNaG-AYDETIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = elmo(\n",
        "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcRIpbk8NIfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(embeddings)\n",
        "embeddings.shape\n",
        "#type(embeddings[0,0,0])\n",
        "#print(embeddings[0,0,0])\n",
        "print(embeddings.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5vRkzi9EVlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
        "                [\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
        "tokens_length = [6, 5]\n",
        "embeddings = elmo(\n",
        "    inputs={\n",
        "        \"tokens\": tokens_input,\n",
        "        \"sequence_len\": tokens_length\n",
        "    },\n",
        "    signature=\"tokens\",\n",
        "    as_dict=True)[\"elmo\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym8ammGXF_eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Create graph and finalize (finalizing optional but recommended).\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  # We will be feeding 1D tensors of text into the graph.\n",
        "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
        "  embedded_text = embed(text_input)\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n",
        "\n",
        "# Create session and initialize.\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YmdPuDiGNxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = session.run(embedded_text, feed_dict={text_input: [\"Hello world\"]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSHUPsh1GPsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G9C0V-hEjtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "#help(elmo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBAQccLIKrc8",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "\n",
        "[^1]: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBv8kSvUE_ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}