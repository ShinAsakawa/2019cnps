{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019cnps_sketch_RNN_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_sketch_RNN_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tDybPQiEFQuJ"
      },
      "source": [
        "# 書画の RNN デモ\n",
        "\n",
        "Python 2.7 required!\n",
        "\n",
        "<!--In this notebook, we will show how to load pre-trained models and draw things with sketch-rnn-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k0GqvYgB9JLC",
        "colab": {}
      },
      "source": [
        "# Sketch_RNN is required to run on python 2.7!\n",
        "# import the required libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import cPickle\n",
        "import codecs\n",
        "import collections\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from six.moves import xrange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UI4ZC__4FQuL",
        "colab": {}
      },
      "source": [
        "# libraries required for visualisation:\n",
        "from IPython.display import SVG, display\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set numpy output to something sensible\n",
        "np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4xYY-TUd9aiD",
        "outputId": "816953c4-ee7f-4947-f13b-93e65f4751c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#!conda install -c omnia svgwrite=1.1.6\n",
        "!pip install svgwrite==1.1.6\n",
        "import svgwrite "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: svgwrite==1.1.6 in /usr/local/lib/python2.7/dist-packages (1.1.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from svgwrite==1.1.6) (2.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NzPSD-XRFQuP",
        "colab": {}
      },
      "source": [
        "tf.logging.info(\"TensorFlow Version: %s\", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NkFS0E1zFQuU",
        "outputId": "6aec3724-87a5-4c24-f2f2-970b78d60f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# import our command line tools\n",
        "from magenta.models.sketch_rnn.sketch_rnn_train import *\n",
        "from magenta.models.sketch_rnn.model import *\n",
        "from magenta.models.sketch_rnn.utils import *\n",
        "from magenta.models.sketch_rnn.rnn import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 08:14:03.359262 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/pipelines/statistics.py:131: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0815 08:14:06.885198 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/music/note_sequence_io.py:60: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0815 08:14:08.676728 140345665382272 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0815 08:14:08.678932 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0815 08:14:08.683971 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:35: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBde4xkEFQuX",
        "colab": {}
      },
      "source": [
        "# little function that displays vector images and saves them to .svg\n",
        "#def draw_strokes(data, factor=0.2, svg_filename = '/tmp/sketch_rnn/svg/sample.svg'):\n",
        "def draw_strokes(data, \n",
        "                 factor=0.2, \n",
        "                 svg_filename='/Users/asakawa/study/2018cnps/jupyter-notebooks/tmp/sketch_rnn/svg/sample.svg'):\n",
        "  tf.gfile.MakeDirs(os.path.dirname(svg_filename))\n",
        "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
        "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
        "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
        "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
        "  lift_pen = 1\n",
        "  abs_x = 25 - min_x \n",
        "  abs_y = 25 - min_y\n",
        "  p = \"M%s,%s \" % (abs_x, abs_y)\n",
        "  command = \"m\"\n",
        "  for i in xrange(len(data)):\n",
        "    if (lift_pen == 1):\n",
        "      command = \"m\"\n",
        "    elif (command != \"l\"):\n",
        "      command = \"l\"\n",
        "    else:\n",
        "      command = \"\"\n",
        "    x = float(data[i,0])/factor\n",
        "    y = float(data[i,1])/factor\n",
        "    lift_pen = data[i, 2]\n",
        "    p += command+str(x)+\",\"+str(y)+\" \"\n",
        "  the_color = \"black\"\n",
        "  stroke_width = 1\n",
        "  dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
        "  dwg.save()\n",
        "  display(SVG(dwg.tostring()))\n",
        "\n",
        "# generate a 2D grid of many vector drawings\n",
        "def make_grid_svg(s_list, grid_space=10.0, grid_space_x=16.0):\n",
        "  def get_start_and_end(x):\n",
        "    x = np.array(x)\n",
        "    x = x[:, 0:2]\n",
        "    x_start = x[0]\n",
        "    x_end = x.sum(axis=0)\n",
        "    x = x.cumsum(axis=0)\n",
        "    x_max = x.max(axis=0)\n",
        "    x_min = x.min(axis=0)\n",
        "    center_loc = (x_max+x_min)*0.5\n",
        "    return x_start-center_loc, x_end\n",
        "  x_pos = 0.0\n",
        "  y_pos = 0.0\n",
        "  result = [[x_pos, y_pos, 1]]\n",
        "  for sample in s_list:\n",
        "    s = sample[0]\n",
        "    grid_loc = sample[1]\n",
        "    grid_y = grid_loc[0]*grid_space+grid_space*0.5\n",
        "    grid_x = grid_loc[1]*grid_space_x+grid_space_x*0.5\n",
        "    start_loc, delta_pos = get_start_and_end(s)\n",
        "\n",
        "    loc_x = start_loc[0]\n",
        "    loc_y = start_loc[1]\n",
        "    new_x_pos = grid_x+loc_x\n",
        "    new_y_pos = grid_y+loc_y\n",
        "    result.append([new_x_pos-x_pos, new_y_pos-y_pos, 0])\n",
        "\n",
        "    result += s.tolist()\n",
        "    result[-1][2] = 1\n",
        "    x_pos = new_x_pos+delta_pos[0]\n",
        "    y_pos = new_y_pos+delta_pos[1]\n",
        "  return np.array(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxrUK4ZxVf4A",
        "colab_type": "text"
      },
      "source": [
        "# まずは「ヒツジ」を描かせてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-yynlhmVf4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/'\n",
        "models_root_dir = '/Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models'\n",
        "model_dir = '/Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models/aaron_sheep/layer_norm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eaSqI0fIFQub",
        "outputId": "5862f8f1-6344-45b0-ee0f-ef91cb1e67a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "download_pretrained_models(models_root_dir=models_root_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0815 08:14:21.414820 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:98: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "I0815 08:14:21.424583 140345665382272 sketch_rnn_train.py:105] Downloading pretrained models from http://download.magenta.tensorflow.org/models/sketch_rnn.zip...\n",
            "I0815 08:14:23.171777 140345665382272 sketch_rnn_train.py:107] Download complete.\n",
            "I0815 08:14:23.173125 140345665382272 sketch_rnn_train.py:108] Unzipping /Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models/sketch_rnn.zip...\n",
            "I0815 08:14:25.269222 140345665382272 sketch_rnn_train.py:111] Unzipping complete.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G4sRuxyn_1aO",
        "colab": {}
      },
      "source": [
        "def load_env_compatible(data_dir, model_dir):\n",
        "  \"\"\"Loads environment for inference mode, used in jupyter notebook.\"\"\"\n",
        "  # modified https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/sketch_rnn_train.py\n",
        "  # to work with depreciated tf.HParams functionality\n",
        "  model_params = sketch_rnn_model.get_default_hparams()\n",
        "  with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
        "    data = json.load(f)\n",
        "  fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\n",
        "  for fix in fix_list:\n",
        "    data[fix] = (data[fix] == 1)\n",
        "  model_params.parse_json(json.dumps(data))\n",
        "  return load_dataset(data_dir, model_params, inference_mode=True)\n",
        "\n",
        "def load_model_compatible(model_dir):\n",
        "  \"\"\"Loads model for inference mode, used in jupyter notebook.\"\"\"\n",
        "  # modified https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/sketch_rnn_train.py\n",
        "  # to work with depreciated tf.HParams functionality\n",
        "  model_params = sketch_rnn_model.get_default_hparams()\n",
        "  with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
        "    data = json.load(f)\n",
        "  fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\n",
        "  for fix in fix_list:\n",
        "    data[fix] = (data[fix] == 1)\n",
        "  model_params.parse_json(json.dumps(data))\n",
        "\n",
        "  model_params.batch_size = 1  # only sample one at a time\n",
        "  eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n",
        "  eval_model_params.use_input_dropout = 0\n",
        "  eval_model_params.use_recurrent_dropout = 0\n",
        "  eval_model_params.use_output_dropout = 0\n",
        "  eval_model_params.is_training = 0\n",
        "  sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n",
        "  sample_model_params.max_seq_len = 1  # sample one point at a time\n",
        "  return [model_params, eval_model_params, sample_model_params]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9m-jSAb3FQuf",
        "outputId": "33a3fd01-eb3f-4376-a67c-61302f3edd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        }
      },
      "source": [
        "[train_set, \n",
        " valid_set, \n",
        " test_set, \n",
        " hps_model, \n",
        " eval_hps_model, \n",
        " sample_hps_model] = load_env_compatible(data_dir, model_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0815 08:14:35.243695 140345665382272 sketch_rnn_train.py:133] Downloading http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-27719b7520ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0mhps_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0meval_hps_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m  sample_hps_model] = load_env_compatible(data_dir, model_dir)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-80f80cf4d671>\u001b[0m in \u001b[0;36mload_env_compatible\u001b[0;34m(data_dir, model_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.pyc\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(data_dir, model_params, inference_mode)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     tf.logging.info('Loaded {}/{}/{} from {}'.format(\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         dataset))\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_strokes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/format.pyc\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    697\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1pHS8TSgFQui",
        "outputId": "ff642c21-9c44-411d-cefa-5681eeb385db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# construct the sketch-rnn model here:\n",
        "reset_graph()\n",
        "model = Model(hps_model)\n",
        "eval_model = Model(eval_hps_model, reuse=True)\n",
        "sample_model = Model(sample_hps_model, reuse=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0815 08:15:11.793584 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:63: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0815 08:15:11.795841 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:66: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c198990b9816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0meval_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_hps_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_hps_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hps_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1gxYLPTQFQuk",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVlDyfN_FQum",
        "outputId": "26575761-b5a5-4c87-dc4e-0eada3ce261a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# loads the weights from checkpoint into our model\n",
        "load_checkpoint(sess, model_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0815 08:15:15.824089 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:240: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0815 08:15:15.825531 140345665382272 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.py:240: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-54ebc5e3f0c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/magenta/models/sketch_rnn/sketch_rnn_train.pyc\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(sess, checkpoint_path)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m   \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m   \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading model %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    823\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    824\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No variables to save"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EOblwpFeFQuq"
      },
      "source": [
        "<!--- We define two convenience functions to encode a stroke into a latent vector, and decode from latent vector to stroke.--->\n",
        "\n",
        "# 動画表象の符号化関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMFlV487FQur",
        "colab": {}
      },
      "source": [
        "def encode(input_strokes):\n",
        "  strokes = to_big_strokes(input_strokes).tolist()\n",
        "  strokes.insert(0, [0, 0, 1, 0, 0])\n",
        "  seq_len = [len(input_strokes)]\n",
        "  draw_strokes(to_normal_strokes(np.array(strokes)))\n",
        "  return sess.run(eval_model.batch_z, feed_dict={eval_model.input_data: [strokes], eval_model.sequence_lengths: seq_len})[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIB2Cs99Vf4T",
        "colab_type": "text"
      },
      "source": [
        "# 動画表象の復号化関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1D5CV7ZlFQut",
        "colab": {}
      },
      "source": [
        "def decode(z_input=None, draw_mode=True, temperature=0.1, factor=0.2):\n",
        "  z = None\n",
        "  if z_input is not None:\n",
        "    z = [z_input]\n",
        "  sample_strokes, m = sample(sess, sample_model, seq_len=eval_model.hps.max_seq_len, temperature=temperature, z=z)\n",
        "  strokes = to_normal_strokes(sample_strokes)\n",
        "  if draw_mode:\n",
        "    draw_strokes(strokes, factor)\n",
        "  return strokes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60dxeQX1Vf4V",
        "colab_type": "text"
      },
      "source": [
        "# では，実際に描いてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUOAvRQtFQuw",
        "outputId": "60d3c8ab-d8cc-4c3a-a43e-d906850ab1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "# get a sample drawing from the test set, and render it to .svg\n",
        "stroke = test_set.random_sample()\n",
        "draw_strokes(stroke)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7c622b69d901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstroke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdraw_strokes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmOCRCHMVf4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j114Re2JFQuz"
      },
      "source": [
        "<!--- Let's try to encode the sample stroke into latent vector $z$-->\n",
        "\n",
        "# 潜在空間上の変数 $z$ を連続的に変化させてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBRjPBo-FQu0",
        "outputId": "0a6a99b9-1178-4988-e98b-b1ba59cb4483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "z = encode(stroke)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2ab908ed616d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stroke' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-37v6eZLFQu5",
        "outputId": "368b90bd-de88-4159-c1d5-4780ccda80f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "_ = decode(z, temperature=0.8) # convert z back to drawing at temperature of 0.8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7f8e49f694b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert z back to drawing at temperature of 0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M5ft6IEBFQu9"
      },
      "source": [
        "Create generated grid at various temperatures from 0.1 to 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BuhaZI0aFQu9",
        "outputId": "13b27755-e6d7-4a1e-c4e2-958177b830ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "stroke_list = []\n",
        "for i in range(10):\n",
        "  stroke_list.append([decode(z, draw_mode=False, temperature=0.1*i+0.1), [0, i]])\n",
        "stroke_grid = make_grid_svg(stroke_list)\n",
        "draw_strokes(stroke_grid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-21d03dbf7ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstroke_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mstroke_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstroke_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid_svg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdraw_strokes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4xiwp3_DFQvB"
      },
      "source": [
        "<!--- Latent Space Interpolation Example between $z_0$ and $z_1$--->\n",
        "\n",
        "# 潜在変数 $z_0$ から $z_1$ の間の線形内挿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSX0uvZTFQvD",
        "outputId": "d6f500a5-99d0-408f-e1dd-9563537ed50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "# get a sample drawing from the test set, and render it to .svg\n",
        "z0 = z\n",
        "_ = decode(z0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5e41c3a19000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jQf99TxOFQvH",
        "outputId": "8b9e7fee-6185-4bb7-855a-a43c58cb547e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "stroke = test_set.random_sample()\n",
        "z1 = encode(stroke)\n",
        "_ = decode(z1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4a1f0dbf1220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstroke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tDqJR8_eFQvK"
      },
      "source": [
        "<!--- Now we interpolate between sheep $z_0$ and sheep $z_1$ --->\n",
        "\n",
        "# 上の2つの間を内挿してみると"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YkPNL5SFQvL",
        "outputId": "95231722-e7c4-4134-b33f-82d70da636f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "z_list = [] # interpolate spherically between z0 and z1\n",
        "N = 10\n",
        "for t in np.linspace(0, 1, N):\n",
        "  z_list.append(slerp(z0, z1, t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0c7eb4153ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mz_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslerp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'z0' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UoM-W1tQFQvM",
        "outputId": "6509582c-753b-4b8c-8f5b-563e72d4f501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "# for every latent vector in z_list, sample a vector image\n",
        "reconstructions = []\n",
        "for i in range(N):\n",
        "  reconstructions.append([decode(z_list[i], draw_mode=False), [0, i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6bfce209a990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreconstructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mreconstructions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTqmlL6GFQvQ",
        "outputId": "56344cdc-55ed-4161-c478-709d8ba9b634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "stroke_grid = make_grid_svg(reconstructions)\n",
        "draw_strokes(stroke_grid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"50\" version=\"1.1\" width=\"50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"50\" width=\"50\" x=\"0\" y=\"0\"/><path d=\"M25,25 m0.0,0.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vFwPna6uFQvS"
      },
      "source": [
        "<!-- Let's load the Flamingo Model, and try Unconditional (Decoder-Only) Generation-->\n",
        "\n",
        "# 別の画像を使ってみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HH-YclgNFQvT",
        "colab": {}
      },
      "source": [
        "model_dir = '/tmp/sketch_rnn/models/flamingo/lstm_uncond'\n",
        "model_dir = '/Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models/flamingo/lstm_uncond'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Znvy3KxFQvU",
        "colab": {}
      },
      "source": [
        "[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cqDNK1cYFQvZ",
        "colab": {}
      },
      "source": [
        "# construct the sketch-rnn model here:\n",
        "reset_graph()\n",
        "model = Model(hps_model)\n",
        "eval_model = Model(eval_hps_model, reuse=True)\n",
        "sample_model = Model(sample_hps_model, reuse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7wzerSI6FQvd",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6mzk8KjOFQvf",
        "colab": {}
      },
      "source": [
        "# loads the weights from checkpoint into our model\n",
        "load_checkpoint(sess, model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X88CgcyuFQvh",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# randomly unconditionally generate 10 examples\n",
        "N = 10\n",
        "reconstructions = []\n",
        "for i in range(N):\n",
        "  #reconstructions.append([decode(temperature=0.5, draw_mode=False), [0, i]])\n",
        "  reconstructions.append([decode(temperature=0.1, draw_mode=False), [0, i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k57REtd_FQvj",
        "colab": {}
      },
      "source": [
        "stroke_grid = make_grid_svg(reconstructions)\n",
        "draw_strokes(stroke_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L-rJ0iUQFQvl"
      },
      "source": [
        "<!--- Let's load the owl model, and generate two sketches using two random IID gaussian latent vectors-->\n",
        "\n",
        "# フクロウと別の画像との融合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "of4SWwGdFQvm",
        "colab": {}
      },
      "source": [
        "model_dir = '/tmp/sketch_rnn/models/owl/lstm'\n",
        "model_dir = '/Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models/owl/lstm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jJiSZFQeFQvp",
        "colab": {}
      },
      "source": [
        "[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n",
        "# construct the sketch-rnn model here:\n",
        "reset_graph()\n",
        "model = Model(hps_model)\n",
        "eval_model = Model(eval_hps_model, reuse=True)\n",
        "sample_model = Model(sample_hps_model, reuse=True)\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# loads the weights from checkpoint into our model\n",
        "load_checkpoint(sess, model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR4TDoi5FQvr",
        "colab": {}
      },
      "source": [
        "z_0 = np.random.randn(eval_model.hps.z_size)\n",
        "_ = decode(z_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZX23lTnpFQvt",
        "colab": {}
      },
      "source": [
        "z_1 = np.random.randn(eval_model.hps.z_size)\n",
        "_ = decode(z_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FjQsF_2FQvv"
      },
      "source": [
        "Let's interpolate between the two owls $z_0$ and $z_1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u6G37E8_FQvw",
        "colab": {}
      },
      "source": [
        "z_list = [] # interpolate spherically between z_0 and z_1\n",
        "N = 10\n",
        "for t in np.linspace(0, 1, N):\n",
        "  z_list.append(slerp(z_0, z_1, t))\n",
        "# for every latent vector in z_list, sample a vector image\n",
        "reconstructions = []\n",
        "for i in range(N):\n",
        "  reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.1), [0, i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OULjMktmFQvx",
        "colab": {}
      },
      "source": [
        "stroke_grid = make_grid_svg(reconstructions)\n",
        "draw_strokes(stroke_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OiXNC-YsFQv0"
      },
      "source": [
        "<!--Let's load the model trained on both cats and buses!  catbus!-->\n",
        "\n",
        "# ネコとバスの画像の融合 インスパイアードバイ ジブリ lol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SL7WpDDQFQv0",
        "colab": {}
      },
      "source": [
        "model_dir = '/tmp/sketch_rnn/models/catbus/lstm'\n",
        "model_dir = '/Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models/catbus/lstm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cvk5WOqHFQv2",
        "colab": {}
      },
      "source": [
        "[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n",
        "# construct the sketch-rnn model here:\n",
        "reset_graph()\n",
        "model = Model(hps_model)\n",
        "eval_model = Model(eval_hps_model, reuse=True)\n",
        "sample_model = Model(sample_hps_model, reuse=True)\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# loads the weights from checkpoint into our model\n",
        "load_checkpoint(sess, model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3VBjUlvVf5O",
        "colab_type": "text"
      },
      "source": [
        "# ネコ画像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "icvlBPVkFQv5",
        "colab": {}
      },
      "source": [
        "z_1 = np.random.randn(eval_model.hps.z_size)\n",
        "_ = decode(z_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uaNxd0LuFQv-",
        "colab": {}
      },
      "source": [
        "z_0 = np.random.randn(eval_model.hps.z_size)\n",
        "_ = decode(z_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VtSYkS6mFQwC"
      },
      "source": [
        "<!-- Let's interpolate between a cat and a bus!!!-->\n",
        "\n",
        "# ではやってみよう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qIDYUxBEFQwD",
        "colab": {}
      },
      "source": [
        "z_list = [] # interpolate spherically between z_1 and z_0\n",
        "N = 10\n",
        "for t in np.linspace(0, 1, N):\n",
        "  z_list.append(slerp(z_1, z_0, t))\n",
        "# for every latent vector in z_list, sample a vector image\n",
        "reconstructions = []\n",
        "for i in range(N):\n",
        "  reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.15), [0, i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZHmnSjSaFQwH",
        "colab": {}
      },
      "source": [
        "stroke_grid = make_grid_svg(reconstructions)\n",
        "draw_strokes(stroke_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "flZ_OgzCFQwJ"
      },
      "source": [
        "<!-- Why stop here? Let's load the model trained on both elephants and pigs!!!-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S8WwK8FPFQwK",
        "colab": {}
      },
      "source": [
        "model_dir = '/tmp/sketch_rnn/models/elephantpig/lstm'\n",
        "model_dir = '/Users/asakawa/study/2018tensorflow_magenta-demos.git/jupyter-notebooks/sketch_rnn/models/elephantpig/lstm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "meOH4AFXFQwM",
        "colab": {}
      },
      "source": [
        "[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n",
        "# construct the sketch-rnn model here:\n",
        "reset_graph()\n",
        "model = Model(hps_model)\n",
        "eval_model = Model(eval_hps_model, reuse=True)\n",
        "sample_model = Model(sample_hps_model, reuse=True)\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# loads the weights from checkpoint into our model\n",
        "load_checkpoint(sess, model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "foZiiYPdFQwO",
        "colab": {}
      },
      "source": [
        "z_0 = np.random.randn(eval_model.hps.z_size)\n",
        "_ = decode(z_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Gaz3QG1FQwS",
        "colab": {}
      },
      "source": [
        "z_1 = np.random.randn(eval_model.hps.z_size)\n",
        "_ = decode(z_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oVtr7NnGFQwU"
      },
      "source": [
        "<!-- Tribute to an episode of [South Park](https://en.wikipedia.org/wiki/An_Elephant_Makes_Love_to_a_Pig): The interpolation between an Elephant and a Pig-->\n",
        "\n",
        "# 「サウスパーク」の `An_Elephant_Makes_Love_to_a_Pig` を使って..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJs9JbROFQwU",
        "colab": {}
      },
      "source": [
        "z_list = [] # interpolate spherically between z_1 and z_0\n",
        "N = 10\n",
        "for t in np.linspace(0, 1, N):\n",
        "  z_list.append(slerp(z_0, z_1, t))\n",
        "# for every latent vector in z_list, sample a vector image\n",
        "reconstructions = []\n",
        "for i in range(N):\n",
        "  #reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.15), [0, i]])\n",
        "  reconstructions.append([decode(z_list[i], draw_mode=False, temperature=0.5), [0, i]])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}