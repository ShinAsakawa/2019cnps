{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019cnps_addtion_rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmTi6dN9Nn9p"
      },
      "source": [
        "# Keras addtion rnn demo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyKVWd5s-iuK"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''An implementation of sequence to sequence learning for performing addition\n",
        "Input: \"535+61\"\n",
        "Output: \"596\"\n",
        "Padding is handled by using a repeated sentinel character (space)\n",
        "\n",
        "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
        "\"Learning to Execute\"\n",
        "http://arxiv.org/abs/1410.4615\n",
        "and\n",
        "\"Sequence to Sequence Learning with Neural Networks\"\n",
        "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
        "Theoretically it introduces shorter term dependencies between source and target.\n",
        "\n",
        "Two digits inverted:\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits inverted:\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits inverted:\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits inverted:\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "from six.moves import range"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLI6yoGe-iuR"
      },
      "source": [
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one hot integer representation\n",
        "    + Decode the one hot integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One hot encode given string C.\n",
        "\n",
        "        # Arguments\n",
        "            num_rows: Number of rows in the returned one hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu5w9zuA-iuU"
      },
      "source": [
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "INVERT = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ivig-G-iue"
      },
      "source": [
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('Generating data...')\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    if INVERT:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print('Total addition questions:', len(questions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXyg6INT-iuk"
      },
      "source": [
        "print('Vectorization...')\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA9FvRLm-iuo"
      },
      "source": [
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQjkK6p-ius"
      },
      "source": [
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('Training Data:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print('Validation Data:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPXGhQJM-iux"
      },
      "source": [
        "# Try replacing GRU, or SimpleRNN.\n",
        "RNN = layers.LSTM\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "LAYERS = 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zaTTDf3-ivD"
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(LAYERS):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
        "model.add(layers.Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3vy6Lvk-ivI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1189bd22-4e33-4465-bf70-59d84a6e07dd"
      },
      "source": [
        "# Train the model each generation and show predictions against the validation dataset.\n",
        "for iteration in range(1, 200):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              validation_data=(x_val, y_val))\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "\n",
        "        preds = model.predict(rowx, verbose=0)\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        #preds = model.predict_classes(rowx, verbose=0)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q[::-1] if INVERT else q)\n",
        "        print('T', correct)\n",
        "        if correct == guess:\n",
        "            print(colors.ok + '☑' + colors.close, end=\" \")\n",
        "        else:\n",
        "            print(colors.fail + '☒' + colors.close, end=\" \")\n",
        "        print(guess)\n",
        "        print('---')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 11s 30ms/step - loss: 0.0087 - accuracy: 0.9989 - val_loss: 0.0081 - val_accuracy: 0.9988\n",
            "Q 810+685\n",
            "T 1495\n",
            "\u001b[91m☒\u001b[0m 1+0 +++11100\n",
            "---\n",
            "Q 981+5  \n",
            "T 986 \n",
            "\u001b[91m☒\u001b[0m 1    00000+ \n",
            "---\n",
            "Q 24+205 \n",
            "T 229 \n",
            "\u001b[91m☒\u001b[0m 1 0+ ++  000\n",
            "---\n",
            "Q 823+402\n",
            "T 1225\n",
            "\u001b[91m☒\u001b[0m 11+ +011111 \n",
            "---\n",
            "Q 84+31  \n",
            "T 115 \n",
            "\u001b[91m☒\u001b[0m 1++ ++00000 \n",
            "---\n",
            "Q 301+80 \n",
            "T 381 \n",
            "\u001b[91m☒\u001b[0m 1+000   ++++\n",
            "---\n",
            "Q 406+0  \n",
            "T 406 \n",
            "\u001b[91m☒\u001b[0m 1 +++  0000+\n",
            "---\n",
            "Q 60+527 \n",
            "T 587 \n",
            "\u001b[91m☒\u001b[0m 1 +  1  00++\n",
            "---\n",
            "Q 618+438\n",
            "T 1056\n",
            "\u001b[91m☒\u001b[0m 1++ ++0011  \n",
            "---\n",
            "Q 26+185 \n",
            "T 211 \n",
            "\u001b[91m☒\u001b[0m 1 1+       +\n",
            "---\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 47\n",
            "352/352 [==============================] - 10s 29ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 0.0071 - val_accuracy: 0.9989\n",
            "Q 725+41 \n",
            "T 766 \n",
            "\u001b[91m☒\u001b[0m 1 1    0+   \n",
            "---\n",
            "Q 680+82 \n",
            "T 762 \n",
            "\u001b[91m☒\u001b[0m 1  000+++   \n",
            "---\n",
            "Q 226+983\n",
            "T 1209\n",
            "\u001b[91m☒\u001b[0m 1+0 ++++1111\n",
            "---\n",
            "Q 941+60 \n",
            "T 1001\n",
            "\u001b[91m☒\u001b[0m 1+011+++++  \n",
            "---\n",
            "Q 258+523\n",
            "T 781 \n",
            "\u001b[91m☒\u001b[0m 1 0000    ++\n",
            "---\n",
            "Q 508+44 \n",
            "T 552 \n",
            "\u001b[91m☒\u001b[0m 1 0000  ++ 0\n",
            "---\n",
            "Q 29+213 \n",
            "T 242 \n",
            "\u001b[91m☒\u001b[0m 1 00 0++++ 0\n",
            "---\n",
            "Q 83+629 \n",
            "T 712 \n",
            "\u001b[91m☒\u001b[0m 1 ++000     \n",
            "---\n",
            "Q 69+688 \n",
            "T 757 \n",
            "\u001b[91m☒\u001b[0m 1++  +++0 00\n",
            "---\n",
            "Q 920+743\n",
            "T 1663\n",
            "\u001b[91m☒\u001b[0m 1+1 111++00 \n",
            "---\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 48\n",
            " 83/352 [======>.......................] - ETA: 7s - loss: 0.0040 - accuracy: 0.9998"
          ]
        }
      ]
    }
  ]
}